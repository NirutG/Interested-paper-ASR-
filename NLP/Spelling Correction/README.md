Interted paper1 : https://ieeexplore.ieee.org/document/9145483


<table class="tg">
<thead>
  <tr>
    <th class="tg-0pky"><span style="font-weight:bold">Title</span></th>
    <th class="tg-0pky">Thai Spelling Correction and Word Normalization on Social Text Using a Two-Stage Pipeline With Neural Contextual Attention</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky"><span style="font-weight:bold">Challenge</span></td>
    <td class="tg-0pky">Text correction system in the task of performing spelling correction and word normalization (text correction) for Thai social media</td>
  </tr>
  <tr>
    <td class="tg-0pky"></td>
    <td class="tg-0pky">text has remained largely unexplored.</td>
  </tr>
  <tr>
    <td class="tg-0lax"></td>
    <td class="tg-0lax">1. Correcting errors</td>
  </tr>
  <tr>
    <td class="tg-0lax"></td>
    <td class="tg-0lax">2. Word variances</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:bold">Key Related Work</span></td>
    <td class="tg-0pky">1. WER : Word Error Rate</td>
  </tr>
  <tr>
    <td class="tg-0pky"></td>
    <td class="tg-0pky">2. GLEU : Generalized Language Evaluation Understanding</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:bold">Solution</span></td>
    <td class="tg-0pky">1. Neural-based text corrector with two-stage structure to alleviate issues of overcorrections</td>
  </tr>
  <tr>
    <td class="tg-0pky"></td>
    <td class="tg-0pky">2. Neural-based error detector</td>
  </tr>
  <tr>
    <td class="tg-0pky"></td>
    <td class="tg-0pky">3. Seq2Seq neural error corrector with contextual attention</td>
  </tr>
  <tr>
    <td class="tg-0lax"></td>
    <td class="tg-0lax">4. Corrections based on both the erroneous text and its context without the need for an end-to-end structure</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:bold">Results</span></td>
    <td class="tg-0pky">1. Outperformed all the evaluated text correction systems</td>
  </tr>
  <tr>
    <td class="tg-0pky"></td>
    <td class="tg-0pky">2. Compared to the second-best result (copy-augmented transformer), our method further reduced WER from 2.51% to 2.07%</td>
  </tr>
  <tr>
    <td class="tg-0pky"></td>
    <td class="tg-0pky">3. Improved the GLEU score from 0.9409 to 0.9502 on the Thai text correction task</td>
  </tr>
  <tr>
    <td class="tg-0lax"></td>
    <td class="tg-0lax">4. Improved the GLEU score from 0.7409 to 0.7539 on the English spelling correction task.</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:bold">Conclusion</span></td>
    <td class="tg-0pky">Proposed system outperformed all the existing tested techniques on the TC task on the Thai UGWC dataset.</td>
  </tr>
  <tr>
    <td class="tg-0lax"></td>
    <td class="tg-0lax">Proposed method also outperformed the state-of-the-art GEC model on an English spelling correction task constructed from the</td>
  </tr>
  <tr>
    <td class="tg-0lax"></td>
    <td class="tg-0lax">Conll2014 task</td>
  </tr>
</tbody>
</table>
